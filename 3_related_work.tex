\chapter{Related Work} \label{ch:related_work}

This chapter surveys the relevant literature underpinning this thesis. Reflecting the structure of the research questions posed in this thesis, the chapter is divided into three main sections. First, we examine foundational aspects of autoencoder design and the influence of architectural and hyperparameter choices. Second, we explore how various loss functions contribute to preserving the structural integrity of the data in the latent space, that is, maintaining meaningful relationships and patterns present in the original data after compression. Third, we review supervised autoencoder approaches that incorporate label information to enhance the discriminative quality of the learned embeddings. This tripartite structure provides the conceptual grounding necessary for the subsequent methods and experimental analyses.

\section{Architectures Factors of Autoencoders}

In the context of autoencoder (AE) design, understanding the architecture, hyperparameter tuning, and design principles is crucial for optimizing performance across various tasks. AEs are typically structured as artificial neural networks comprising two principal components: the encoder, which compresses the input data into a latent representation, and the decoder, which reconstructs the input from this latent code. The architectural choices and hyperparameter configurations of these networks directly influence their representational capacity, robustness, and generalization ability.

The fundamental structure of an autoencoder can vary from shallow models, with a single hidden layer acting as the bottleneck, to deep configurations that incorporate multiple hidden layers in both encoder and decoder segments. Deeper architectures are generally more expressive and capable of learning complex hierarchical representations \cite{Goodfellow16}. However, this increase in depth necessitates careful consideration of design aspects such as the choice of activation functions, weight initialization, and optimization algorithms to mitigate challenges like vanishing gradients and overfitting.

In terms of activation functions, traditional autoencoders often utilize sigmoid or tanh functions, especially in shallower networks, due to their bounded outputs and historical precedent. Nevertheless, modern deep AEs frequently adopt rectified linear units (ReLU) \cite{ReLU} and their variants, such as leaky ReLU \cite{leakyReLU} or SELU \cite{SELU}, which promote sparsity and mitigate vanishing gradient issues during training [\cite{Charte18}, \cite{Berahmand24}]. The inclusion of these activation functions often results in more efficient training dynamics and better representation quality.

The dimensionality of the bottleneck layer is a critical architectural parameter. Undercomplete autoencoders, where the latent dimension is smaller than the input, are inherently encouraged to learn compressed, informative representations. Overcomplete architectures, on the other hand, necessitate the application of additional constraints or regularization techniques, such as sparsity or noise injection, to prevent trivial identity mappings and to enforce meaningful encoding [\cite{Goodfellow16}, \cite{Charte18}].

From a hyperparameter perspective, choices such as the number of hidden units per layer, learning rate, batch size, type of optimizer, and regularization strategies significantly impact AE performance. Learning rate selection, in particular, must balance convergence speed and stability, often requiring empirical tuning. Similarly, the batch size can influence both the smoothness of gradient estimates and memory efficiency, necessitating a trade-off that is context-dependent \cite{Berahmand24}.

An additional consideration involves the selection of the loss function. Mean squared error (MSE) is a typical choice for continuous-valued data, while binary cross-entropy is preferred for binary inputs or outputs bounded within the [0,1] interval. The alignment of the loss function with the data characteristics and the overall training objective is imperative for effective learning. Furthermore, the use of advanced optimization algorithms such as Adam \cite{Adam} or RMSprop \cite{RMSProp}, which adapt learning rates for individual parameters, often facilitates more efficient training convergence, particularly in high-dimensional settings [\cite{Goodfellow16}, \cite{Charte18}, \cite{Berahmand24}].

In practical terms, AE design is an iterative and data-dependent process. While general guidelines exist, such as aligning encoder-decoder depths, mirroring layer widths, and gradually reducing the number of units toward the bottleneck, final architecture tuning often benefits from empirical validation through performance metrics \cite{Charte18}.

In summary, the design and tuning of autoencoders involve a nuanced interplay between structural decisions and hyperparameter optimization. Drawing from foundational insights by Goodfellow et al. (2016) \cite{Goodfellow16}, design strategies by Charte et al. (2018) \cite{Charte18}, and evaluations of hyperparameter sensitivity by Berahmand et al. (2024) \cite{Berahmand24}, it becomes evident that effective AE deployment hinges on a principled yet flexible approach.

\section{Loss-based Structure Preservation}

Central to the efficacy of autoencoders is the ability to preserve essential structural information in the data during reconstruction. This characteristic is typically governed by the loss function employed during training, which plays a critical role in ensuring that the encoded representation retains the salient features of the original input. The concept of loss-based structure preservation is particularly crucial when autoencoders are applied in domains where the intrinsic relationships within data must be maintained, such as financial time series forecasting \cite{Bieganowski24}.

A conventional autoencoder consists of an encoder that maps input data to a lower-dimensional latent space and a decoder that reconstructs the input from this compressed representation. The optimization of this process is generally driven by a reconstruction loss, often measured using metrics like mean squared error (MSE) or binary cross-entropy, depending on the nature of the input data. While such basic losses encourage fidelity between the input and its reconstruction, they do not inherently preserve the geometric or topological structure of the input space. Consequently, researchers have extended traditional autoencoder models to incorporate loss terms explicitly designed to preserve local and global structures in the data \cite{Goodfellow16}.

One of the earliest and most influential approaches to incorporating structural information into the autoencoder's loss function is the use of contractive autoencoders, which penalize the Jacobian of the encoderâ€™s output with respect to its input. This method enforces the learned representation to be locally invariant to small perturbations in the input space, thereby preserving local structure \cite{Rifai11}.

The idea of structure preservation has been further refined in manifold learning-inspired variants such as the locally linear embedding (LLE) autoencoder. These models integrate principles from classical manifold learning into the loss function by penalizing discrepancies in local neighborhood relationships between the original data and the latent representations. For example, the Laplacian autoencoder incorporates a graph Laplacian regularization term into the loss function, promoting the preservation of the graph structure induced by nearest-neighbor relationships in the input space [\cite{Miani22}, \cite{Wang14}]. This encourages the autoencoder to maintain local isometry, ensuring that similar inputs remain close in the latent space.

In addition to preserving local structures, several works have focused on global structural preservation, particularly when the autoencoder is applied to data with hierarchical or graph-based relationships. Graph autoencoders, for instance, are designed to learn node embeddings that reflect the structure of the graph, often using reconstruction losses that penalize errors in predicting adjacency relationships or graph Laplacian properties \cite{Kipf16}. These models demonstrate how loss functions can be adapted to handle more complex relational structures, expanding the applicability of autoencoders beyond grid-structured data to network-structured inputs.

The challenge of preserving structure becomes more pronounced in variational autoencoders (VAEs), where the probabilistic nature of the latent space can lead to more diffuse representations. To counteract this, researchers have proposed various modifications to the VAE objective, such as the $\beta$-VAE, which introduces a weighting factor on the Kullback-Leibler divergence term to control the trade-off between reconstruction accuracy and latent space disentanglement \cite{Burgess18}.

In addition to conventional structure-preserving strategies, equipping autoencoders with specialized loss functions tailored for clustering tasks has become an increasingly effective approach to bridge the gap between representation learning and clustering tasks. Classic autoencoders trained solely on reconstruction loss often produce latent spaces that are not inherently conducive to clustering, as they optimize for data fidelity rather than cluster separability. To address this, researchers have developed clustering-specific loss functions that can be incorporated into the training regime of autoencoders to promote latent space organization aligned with clustering objectives [\cite{Leiber25}, \cite{Beer24}].

In summary, the loss-based preservation of structure in autoencoders represents a broad and evolving research area. The design of loss functions that go beyond simple reconstruction error is critical to ensuring that autoencoders produce meaningful and structurally coherent representations. Whether through local neighborhood preservation, graph-based constraints, contrastive objectives, or probabilistic regularization, these approaches collectively aim to align the latent representations with the intrinsic geometry and semantics of the input space. As such, the ongoing refinement of loss functions tailored to structure preservation continues to be a key driver in the advancement of autoencoder architectures and their applications across diverse domains.

\section{Supervised Autoencoders}

Supervised autoencoders represent a sophisticated integration of unsupervised and supervised learning paradigms, combining the representational power of traditional autoencoders with the predictive capabilities of supervised models. While conventional autoencoders are designed to learn efficient low-dimensional representations of data through the task of reconstructing input signals, supervised autoencoders extend this framework by incorporating label information into the learning process. This fusion enables the latent representations to be not only compact and informative but also discriminative with respect to specific supervised objectives \cite{Le18}.

Training a supervised autoencoder involves optimizing a composite loss function that typically includes a reconstruction loss, such as the mean squared error, and a supervised loss. This dual-objective optimization forces the model to balance between reconstructing the input accurately and producing latent features conducive to accurate predictions \cite{Le18}.

One of the key advantages of supervised autoencoders lies in their ability to learn task-specific representations that are more aligned with the predictive objective than those learned by purely unsupervised methods. In many domains, such as medical imaging \cite{Rajpurkar17}, natural language processing \cite{Devlin18}, object detection \cite{Ren16} and time series forecasting \cite{Bieganowski24} incorporating label information during representation learning has been shown to improve downstream performance.

Despite their advantages, supervised autoencoders are not without limitations. One common concern is the potential for the supervised loss to dominate training, leading to latent representations that are highly predictive but lack generative capacity or robustness to noise \cite{Le18}.

A prevalent strategy within supervised autoencoders is the use of metric learning, particularly through the incorporation of triplet loss. This approach has been successfully demonstrated in models like the Triplet Enhanced AutoEncoder (TEA), which reformulates label information as triplets, comprising an anchor, a positive sample from the same class, and a negative sample from a different class, to guide the embedding process. By enforcing that the anchor is closer to the positive than to the negative in the latent space, the model achieves a latent structure that preserves both network topology and class separability without being tied to a specific classifier, a property referred to as being model-free \cite{Yang19}.

Triplet-based supervision has also been extended to generative frameworks such as the Triplet Variational Autoencoder (TVAE). In this approach, the triplet loss is jointly optimized with the Evidence Lower Bound (ELBO) of the VAE, effectively combining reconstruction fidelity, latent regularization, and metric learning to yield more discriminative embeddings. Empirical results indicate that such a hybrid loss structure significantly improves triplet accuracy compared to traditional VAEs \cite{Ishfaq23}.

Beyond triplet loss, other supervised strategies have been proposed to enhance autoencoder-based representation learning. The Supervised COSMOS Autoencoder, for instance, incorporates a multi-objective loss function comprising cosine similarity, Mahalanobis distance, and a mutual information-based supervision term. This architecture is designed to capture both angular and statistical relationships in the data while explicitly promoting class discriminability. The model demonstrates that incorporating supervision can lead to latent spaces that are not only robust to input variations such as illumination and pose but also better aligned with classification objectives \cite{Singh18}.

In summary, supervised autoencoders provide a compelling framework for learning representations that are simultaneously compact, interpretable, and predictive. By integrating supervised objectives into the autoencoding process, these models offer a principled way to incorporate domain knowledge and task-specific signals into representation learning. Their flexibility and adaptability have made them an increasingly popular choice across a wide range of machine learning applications, and ongoing research continues to refine their architectures, training procedures, and theoretical underpinnings.

\section{The Research Gap}

The reviewed literature highlights the crucial influence of both architectural configurations and loss functions on the ability of autoencoders to preserve meaningful structural relationships within data. While prior work has proposed various advanced architectures and structure-aware loss formulations, a systematic investigation into their impact on the preservation of in-between instances (IBIs) remains absent. Building upon these insights, this thesis adopts an experimental approach that evaluates the effects of key architectural choices and loss strategies on IBI preservation.